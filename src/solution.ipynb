{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WZ7Ag3ueXRU",
        "outputId": "20aa1dd0-999a-4cc9-89e9-5c39d5d83e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1046474d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/paddlepaddle-gpu/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x104369430>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/paddlepaddle-gpu/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x104f75730>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/paddlepaddle-gpu/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x104f65e50>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/paddlepaddle-gpu/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x104f67f20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/paddlepaddle-gpu/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement paddlepaddle-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for paddlepaddle-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install paddlepaddle-gpu paddleocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "8-Iy9W1ipgU1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024/09/24 15:04:45] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/hammadkhan/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/hammadkhan/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/opt/anaconda3/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/hammadkhan/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
            "[2024/09/24 15:04:47] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.3538780212402344\n",
            "[2024/09/24 15:04:47] ppocr DEBUG: cls num  : 3, elapsed : 0.027315855026245117\n",
            "[2024/09/24 15:04:47] ppocr DEBUG: rec_res num  : 3, elapsed : 0.41875505447387695\n",
            "[[[[[312.0, 414.0], [419.0, 415.0], [419.0, 429.0], [312.0, 428.0]], ('.41', 0.718310534954071)]]]\n",
            ".41\n",
            "{'text': '.41'}\n",
            ".41\n",
            "Processing image 1 of 20...\n",
            "[2024/09/24 15:04:47] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.04614830017089844\n",
            "[2024/09/24 15:04:47] ppocr DEBUG: cls num  : 6, elapsed : 0.047467947006225586\n",
            "[2024/09/24 15:04:48] ppocr DEBUG: rec_res num  : 6, elapsed : 0.9177179336547852\n",
            "[[[[[283.0, 6.0], [339.0, 6.0], [339.0, 41.0], [283.0, 41.0]], ('Size', 0.9960066080093384)], [[[647.0, 9.0], [716.0, 9.0], [716.0, 38.0], [647.0, 38.0]], ('Width', 0.9995730519294739)], [[[1078.0, 4.0], [1163.0, 8.0], [1161.0, 43.0], [1076.0, 39.0]], ('Length', 0.9998332858085632)], [[[260.0, 54.0], [361.0, 58.0], [360.0, 90.0], [259.0, 86.0]], ('One Size', 0.9805198311805725)], [[[616.0, 56.0], [750.0, 56.0], [750.0, 88.0], [616.0, 88.0]], ('42cm/16.54\"', 0.9613236784934998)], [[[1050.0, 59.0], [1192.0, 59.0], [1192.0, 86.0], [1050.0, 86.0]], ('200cm/78.74\"', 0.9994447231292725)]]]\n",
            "Size\n",
            "Width\n",
            "Length\n",
            "One Size\n",
            "42cm/16.54\"\n",
            "200cm/78.74\"\n",
            "{'text': 'Size\\nWidth\\nLength\\nOne Size\\n42cm/16.54\"\\n200cm/78.74\"'}\n",
            "Size\n",
            "Width\n",
            "Length\n",
            "One Size\n",
            "42cm/16.54\"\n",
            "200cm/78.74\"\n",
            "Processing image 2 of 20...\n",
            "Processing image 3 of 20...\n",
            "Processing image 4 of 20...\n",
            "[2024/09/24 15:04:49] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.04832315444946289\n",
            "[2024/09/24 15:04:49] ppocr DEBUG: cls num  : 6, elapsed : 0.047601938247680664\n",
            "[2024/09/24 15:04:49] ppocr DEBUG: rec_res num  : 6, elapsed : 0.7203588485717773\n",
            "[[[[[281.0, 6.0], [338.0, 6.0], [338.0, 41.0], [281.0, 41.0]], ('SIze', 0.9038881063461304)], [[[1102.0, 8.0], [1183.0, 8.0], [1183.0, 41.0], [1102.0, 41.0]], ('Length', 0.9770975708961487)], [[[259.0, 56.0], [359.0, 56.0], [359.0, 88.0], [259.0, 88.0]], ('One SIze', 0.9001063108444214)], [[[627.0, 58.0], [775.0, 58.0], [775.0, 85.0], [627.0, 85.0]], ('10.50cm/4.13\"', 0.9944142699241638)], [[[1074.0, 54.0], [1207.0, 58.0], [1206.0, 90.0], [1073.0, 86.0]], ('90cm/35.43\"', 0.9919362664222717)]]]\n",
            "SIze\n",
            "Length\n",
            "One SIze\n",
            "10.50cm/4.13\"\n",
            "90cm/35.43\"\n",
            "{'text': 'SIze\\nLength\\nOne SIze\\n10.50cm/4.13\"\\n90cm/35.43\"'}\n",
            "SIze\n",
            "Length\n",
            "One SIze\n",
            "10.50cm/4.13\"\n",
            "90cm/35.43\"\n",
            "Processing image 5 of 20...\n",
            "Processing image 6 of 20...\n",
            "Processing image 7 of 20...\n",
            "[2024/09/24 15:04:50] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.34389519691467285\n",
            "[2024/09/24 15:04:50] ppocr DEBUG: cls num  : 3, elapsed : 0.024679899215698242\n",
            "[2024/09/24 15:04:50] ppocr DEBUG: rec_res num  : 3, elapsed : 0.36214113235473633\n",
            "[[[[[375.0, 414.0], [424.0, 414.0], [424.0, 428.0], [375.0, 428.0]], ('131.1', 0.7315620183944702)]]]\n",
            "131.1\n",
            "{'text': '131.1'}\n",
            "131.1\n",
            "Processing image 8 of 20...\n",
            "[2024/09/24 15:04:51] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.6782698631286621\n",
            "[2024/09/24 15:04:51] ppocr DEBUG: cls num  : 3, elapsed : 0.024096965789794922\n",
            "[2024/09/24 15:04:52] ppocr DEBUG: rec_res num  : 3, elapsed : 0.37085962295532227\n",
            "[[[[[479.0, 417.0], [597.0, 421.0], [596.0, 460.0], [478.0, 456.0]], ('40 cm', 0.96947181224823)], [[[1018.0, 412.0], [1133.0, 416.0], [1132.0, 452.0], [1017.0, 448.0]], ('30 cm', 0.9956258535385132)], [[[1302.0, 702.0], [1421.0, 706.0], [1420.0, 744.0], [1300.0, 740.0]], ('1.5 cm', 0.9530046582221985)]]]\n",
            "40 cm\n",
            "30 cm\n",
            "1.5 cm\n",
            "{'text': '40 cm\\n30 cm\\n1.5 cm'}\n",
            "40 cm\n",
            "30 cm\n",
            "1.5 cm\n",
            "Processing image 9 of 20...\n",
            "[2024/09/24 15:04:53] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.6616029739379883\n",
            "[2024/09/24 15:04:53] ppocr DEBUG: cls num  : 3, elapsed : 0.046723127365112305\n",
            "[2024/09/24 15:04:53] ppocr DEBUG: rec_res num  : 3, elapsed : 0.36258411407470703\n",
            "[[[[[479.0, 417.0], [599.0, 421.0], [598.0, 460.0], [478.0, 456.0]], ('40 cm', 0.935289740562439)], [[[1018.0, 412.0], [1133.0, 416.0], [1132.0, 452.0], [1017.0, 448.0]], ('30 cm', 0.9956985712051392)], [[[1302.0, 700.0], [1421.0, 704.0], [1420.0, 744.0], [1300.0, 740.0]], ('15 cm', 0.9994710683822632)]]]\n",
            "40 cm\n",
            "30 cm\n",
            "15 cm\n",
            "{'text': '40 cm\\n30 cm\\n15 cm'}\n",
            "40 cm\n",
            "30 cm\n",
            "15 cm\n",
            "Processing image 10 of 20...\n",
            "[2024/09/24 15:04:54] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.062455177307128906\n",
            "[2024/09/24 15:04:54] ppocr DEBUG: cls num  : 1, elapsed : 0.009237051010131836\n",
            "[2024/09/24 15:04:54] ppocr DEBUG: rec_res num  : 1, elapsed : 0.12943291664123535\n",
            "[[[[[230.0, 144.0], [243.0, 144.0], [243.0, 158.0], [230.0, 158.0]], ('-', 0.5578268766403198)]]]\n",
            "-\n",
            "{'text': '-'}\n",
            "-\n",
            "Processing image 11 of 20...\n",
            "[2024/09/24 15:04:54] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.12608623504638672\n",
            "[2024/09/24 15:04:54] ppocr DEBUG: cls num  : 3, elapsed : 0.024687767028808594\n",
            "[2024/09/24 15:04:55] ppocr DEBUG: rec_res num  : 3, elapsed : 0.362926721572876\n",
            "[[[[[13.0, 107.0], [81.0, 107.0], [81.0, 128.0], [13.0, 128.0]], ('30.6cm', 0.9992179870605469)], [[[413.0, 204.0], [488.0, 206.0], [488.0, 227.0], [412.0, 225.0]], ('139.5cm', 0.9853727221488953)], [[[67.0, 216.0], [131.0, 219.0], [130.0, 237.0], [66.0, 234.0]], ('41.6cm', 0.984227180480957)]]]\n",
            "30.6cm\n",
            "139.5cm\n",
            "41.6cm\n",
            "{'text': '30.6cm\\n139.5cm\\n41.6cm'}\n",
            "30.6cm\n",
            "139.5cm\n",
            "41.6cm\n",
            "Processing image 12 of 20...\n",
            "[2024/09/24 15:04:55] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.3450438976287842\n",
            "[2024/09/24 15:04:56] ppocr DEBUG: cls num  : 4, elapsed : 0.033049821853637695\n",
            "[2024/09/24 15:04:56] ppocr DEBUG: rec_res num  : 4, elapsed : 0.48159098625183105\n",
            "[[[[[491.0, 450.0], [547.0, 450.0], [547.0, 476.0], [491.0, 476.0]], ('4.3 in', 0.9474470615386963)], [[[491.0, 474.0], [552.0, 476.0], [551.0, 502.0], [490.0, 499.0]], ('10 cm', 0.9325634241104126)]]]\n",
            "4.3 in\n",
            "10 cm\n",
            "{'text': '4.3 in\\n10 cm'}\n",
            "4.3 in\n",
            "10 cm\n",
            "Processing image 13 of 20...\n",
            "Processing image 14 of 20...\n",
            "Processing image 15 of 20...\n",
            "[2024/09/24 15:04:56] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.059419870376586914\n",
            "[2024/09/24 15:04:56] ppocr DEBUG: cls num  : 2, elapsed : 0.017242908477783203\n",
            "[2024/09/24 15:04:57] ppocr DEBUG: rec_res num  : 2, elapsed : 0.2426128387451172\n",
            "[None]\n",
            "\n",
            "{'text': ''}\n",
            "\n",
            "Processing image 16 of 20...\n",
            "[2024/09/24 15:04:57] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.07637572288513184\n",
            "[2024/09/24 15:04:57] ppocr DEBUG: cls num  : 9, elapsed : 0.0706784725189209\n",
            "[2024/09/24 15:04:58] ppocr DEBUG: rec_res num  : 9, elapsed : 1.0883078575134277\n",
            "[[[[[41.0, 292.0], [66.0, 292.0], [66.0, 308.0], [41.0, 308.0]], ('1N', 0.5580917000770569)], [[[41.0, 420.0], [50.0, 420.0], [50.0, 430.0], [41.0, 430.0]], ('1', 0.9789130091667175)], [[[42.0, 468.0], [51.0, 468.0], [51.0, 478.0], [42.0, 478.0]], (':', 0.5398054718971252)], [[[42.0, 540.0], [52.0, 540.0], [52.0, 550.0], [42.0, 550.0]], ('1', 0.9640161395072937)]]]\n",
            "1N\n",
            "1\n",
            ":\n",
            "1\n",
            "{'text': '1N\\n1\\n:\\n1'}\n",
            "1N\n",
            "1\n",
            ":\n",
            "1\n",
            "Processing image 17 of 20...\n",
            "[2024/09/24 15:04:58] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.11490321159362793\n",
            "[2024/09/24 15:04:58] ppocr DEBUG: cls num  : 12, elapsed : 0.09471702575683594\n",
            "[2024/09/24 15:05:00] ppocr DEBUG: rec_res num  : 12, elapsed : 1.4379158020019531\n",
            "[[[[[225.0, 46.0], [288.0, 46.0], [288.0, 76.0], [225.0, 76.0]], ('Black', 0.9977648854255676)], [[[384.0, 46.0], [433.0, 46.0], [433.0, 75.0], [384.0, 75.0]], ('Red', 0.9996905326843262)], [[[535.0, 46.0], [601.0, 50.0], [599.0, 76.0], [534.0, 72.0]], ('Green', 0.9993442296981812)], [[[22.0, 65.0], [123.0, 65.0], [123.0, 90.0], [22.0, 90.0]], ('208-240V', 0.998810350894928)], [[[47.0, 101.0], [98.0, 101.0], [98.0, 128.0], [47.0, 128.0]], ('16A', 0.999017059803009)], [[[44.0, 137.0], [103.0, 140.0], [102.0, 167.0], [42.0, 164.0]], ('60Hz', 0.9682102203369141)], [[[238.0, 141.0], [275.0, 141.0], [275.0, 171.0], [238.0, 171.0]], ('L1', 0.9553954601287842)], [[[390.0, 141.0], [428.0, 141.0], [428.0, 171.0], [390.0, 171.0]], ('L2', 0.9741629958152771)], [[[541.0, 143.0], [595.0, 143.0], [595.0, 170.0], [541.0, 170.0]], ('GND', 0.9876828193664551)], [[[226.0, 174.0], [287.0, 174.0], [287.0, 200.0], [226.0, 200.0]], ('Black', 0.9994432330131531)], [[[377.0, 174.0], [438.0, 174.0], [438.0, 200.0], [377.0, 200.0]], ('White', 0.9997269511222839)], [[[534.0, 172.0], [602.0, 176.0], [600.0, 202.0], [533.0, 198.0]], ('Green', 0.9996925592422485)]]]\n",
            "Black\n",
            "Red\n",
            "Green\n",
            "208-240V\n",
            "16A\n",
            "60Hz\n",
            "L1\n",
            "L2\n",
            "GND\n",
            "Black\n",
            "White\n",
            "Green\n",
            "{'text': 'Black\\nRed\\nGreen\\n208-240V\\n16A\\n60Hz\\nL1\\nL2\\nGND\\nBlack\\nWhite\\nGreen'}\n",
            "Black\n",
            "Red\n",
            "Green\n",
            "208-240V\n",
            "16A\n",
            "60Hz\n",
            "L1\n",
            "L2\n",
            "GND\n",
            "Black\n",
            "White\n",
            "Green\n",
            "Processing image 18 of 20...\n",
            "Processing image 19 of 20...\n",
            "Processing image 20 of 20...\n"
          ]
        }
      ],
      "source": [
        "#Importing the neccesary dependencies and required libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR\n",
        "import math\n",
        "import logging\n",
        "\n",
        "# Configure logging for detailed debug information\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Unit normalization map to replace shorthand units with full forms\n",
        "unit_normalization_map = {\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'lbs': 'pound',\n",
        "    'lb': 'pound',\n",
        "    'ounces': 'ounce',\n",
        "    'oz': 'ounce',\n",
        "    'ml': 'milliliter',\n",
        "    'l': 'liter',\n",
        "    'cup': 'cup',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt',\n",
        "    'cm': 'centimeter',\n",
        "    'mm': 'millimeter',\n",
        "    'inches': 'inch',\n",
        "    'm': 'meter',\n",
        "    'km': 'kilometer',\n",
        "    'foot': 'foot',\n",
        "    'feet': 'foot'\n",
        "}\n",
        "\n",
        "# Reading CSV file\n",
        "def read_csv(file_path):\n",
        "    logging.info(f\"Reading CSV file: {file_path}\")\n",
        "    return pd.read_csv(file_path).head(20)\n",
        "\n",
        "# Normalising the units using the mapping provided above\n",
        "def normalize_unit(unit):\n",
        "    normalized = unit_normalization_map.get(unit.lower(), unit)\n",
        "    logging.debug(f\"Normalized unit '{unit}' to '{normalized}'\")\n",
        "    return normalized\n",
        "\n",
        "# Extracting numeric values without changing the format\n",
        "def format_value(value):\n",
        "    try:\n",
        "        # logging.debug(f\"Extracted value: {value}\")\n",
        "        return str(value)\n",
        "    except ValueError:\n",
        "        # logging.error(f\"ValueError encountered with value: {value}\")\n",
        "        return value\n",
        "\n",
        "# Preprocessing the image to enhance OCR accuracy\n",
        "def preprocess_image(image):\n",
        "    # logging.info(\"Preprocessing image for OCR\")\n",
        "    image = resize_image(image)\n",
        "    image = enhance_contrast(image)\n",
        "    image = binarize_image(image)\n",
        "    image = denoise_image(image)\n",
        "    image = adaptive_threshold(image)\n",
        "    return image\n",
        "\n",
        "# Resizing the image for better OCR accuracy\n",
        "def resize_image(image, scale_factor=1.5):\n",
        "    width, height = image.size\n",
        "    resized_image = image.resize((int(width * scale_factor), int(height * scale_factor)), Image.Resampling.LANCZOS)\n",
        "    # logging.debug(\"Resized image for better OCR accuracy\")\n",
        "    return resized_image\n",
        "\n",
        "# Enhancing the contrast to improve text visibility\n",
        "def enhance_contrast(image):\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    enhanced_image = enhancer.enhance(2)\n",
        "    # logging.debug(\"Enhanced image contrast\")\n",
        "    return enhanced_image\n",
        "\n",
        "# Binarizing the image to simplify OCR (black-and-white)\n",
        "def binarize_image(image):\n",
        "    gray = image.convert('L')\n",
        "    binary = gray.point(lambda x: 0 if x < 128 else 255, '1')\n",
        "    # logging.debug(\"Binarized image for OCR\")\n",
        "    return binary\n",
        "\n",
        "# Denoising the image using Gaussian blur\n",
        "def denoise_image(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    denoised_image = cv2.GaussianBlur(image_np, (5, 5), 0)\n",
        "    # logging.debug(\"Applied Gaussian blur to denoise image\")\n",
        "    return Image.fromarray(denoised_image)\n",
        "\n",
        "# Apply adaptive thresholding to the image\n",
        "def adaptive_threshold(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    thresh_image = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    # logging.debug(\"Applied adaptive thresholding to image\")\n",
        "    return Image.fromarray(thresh_image)\n",
        "\n",
        "# Perform OCR with PaddleOCR\n",
        "def ocr_paddleocr(image, ocr_instance):\n",
        "    try:\n",
        "        result = ocr_instance.ocr(np.array(image), cls=True)\n",
        "        logging.debug(f\"OCR result: {result}\")\n",
        "        print(result)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"OCR processing failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    if not result or not isinstance(result, list):\n",
        "        # logging.warning(\"No OCR result found or invalid result format\")\n",
        "        return \"\"\n",
        "\n",
        "    extracted_text = []\n",
        "    for block in result:\n",
        "        if isinstance(block, list) and len(block) > 0:\n",
        "            for line in block:\n",
        "                if isinstance(line, list) and len(line) > 1:\n",
        "                    text = line[1][0]\n",
        "                    extracted_text.append(text)\n",
        "\n",
        "    extracted_text_str = \"\\n\".join(extracted_text)\n",
        "    logging.debug(f\"Extracted text: {extracted_text_str}\")\n",
        "    return extracted_text_str if extracted_text else \"\"\n",
        "\n",
        "# Running OCR on the image\n",
        "def run_ocr(image, ocr_instance):\n",
        "    # logging.info(\"Running OCR on preprocessed image\")\n",
        "    paddleocr_text = ocr_paddleocr(image, ocr_instance)\n",
        "    print(paddleocr_text)\n",
        "    return {'text': paddleocr_text}\n",
        "\n",
        "# Extracting information using OCR and regex\n",
        "def extract_info(image_url, entity_name, ocr_instance, cache):\n",
        "    if image_url in cache:\n",
        "        # logging.info(f\"Using cached OCR result for URL: {image_url}\")\n",
        "        cleaned_text = cache[image_url]\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            logging.info(f\"Loaded image from URL: {image_url}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load image from {image_url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "        # Preprocessing the image\n",
        "        img = preprocess_image(img)\n",
        "\n",
        "        # Performing OCR\n",
        "        ocr_results = run_ocr(img, ocr_instance)\n",
        "        print(ocr_results)\n",
        "        \n",
        "        cleaned_text = ocr_results['text']\n",
        "        print(cleaned_text)\n",
        "\n",
        "        # Cache the OCR result to improve the prediction times\n",
        "        cache[image_url] = cleaned_text\n",
        "\n",
        "    # Defining regex patterns\n",
        "    regex_patterns = {\n",
        "        'item_weight': r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)',\n",
        "        'item_volume': r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|fl\\.?\\s*oz|cup|liter|millilitre|gallon?)',\n",
        "        'voltage': r'(\\d+(?:\\.\\d+)?)\\s*(v|volt|mv|millivolt)',\n",
        "        'wattage': r'(\\d+(?:\\.\\d+)?)\\s*(w|watt|mw|milliwatt)',\n",
        "        'maximum_weight_recommendation': r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)',\n",
        "        'height': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "        'depth': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "        'width': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "    }\n",
        "\n",
        "    pattern = regex_patterns.get(entity_name)\n",
        "    if pattern:\n",
        "        match = re.search(pattern, cleaned_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            value = format_value(match.group(1))  # Keep the value as is\n",
        "            unit = normalize_unit(match.group(2))\n",
        "            logging.info(f\"Extracted entity {entity_name}: {value} {unit}\")\n",
        "            return f\"{value} {unit}\"\n",
        "\n",
        "    # logging.warning(f\"No match found for entity {entity_name}\")\n",
        "    return \"\"\n",
        "\n",
        "# Predicting entity values and save results to a CSV file\n",
        "def predict_and_save(csv_data, output_file_path, ocr_instance):\n",
        "    predictions = []\n",
        "    total_images = len(csv_data)\n",
        "    cache = {}  # Cache for storing OCR results based on URL\n",
        "\n",
        "    for index, row in csv_data.iterrows():\n",
        "        image_url = row['image_link']\n",
        "        entity_name = row['entity_name']\n",
        "\n",
        "        predicted_value = extract_info(image_url, entity_name, ocr_instance, cache)\n",
        "\n",
        "        if pd.isna(predicted_value) or predicted_value == \"\":\n",
        "            predicted_value = \"\"\n",
        "\n",
        "        # Printing the current image index and how many are left\n",
        "        # logging.info(f\"Processing image {index + 1} of {total_images}, Predicted Value: {predicted_value}\")\n",
        "        print(f\"Processing image {index + 1} of {total_images}...\")\n",
        "\n",
        "        predictions.append({'index': index, 'prediction': predicted_value})\n",
        "\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.to_csv(output_file_path, index=False)\n",
        "    # logging.info(f\"Predicted values saved to {output_file_path}\")\n",
        "\n",
        "# Main function to load data and predict entity values\n",
        "def main():\n",
        "    csv_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test.csv'\n",
        "    output_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test_out_sample.csv'\n",
        "\n",
        "    csv_data = read_csv(csv_file_path)\n",
        "\n",
        "    # Initialising PaddleOCR once\n",
        "    ocr_instance = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n",
        "\n",
        "    # Predicting entity values and save them to the output CSV\n",
        "    predict_and_save(csv_data, output_file_path, ocr_instance)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZZp4sfaT4srX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'This product is 42cm in height' classified as: height\n",
            "Sentence: ' with a width of 50cm and depth of 30cm.' classified as: width\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model for sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Labels: 0 = height, 1 = width, 2 = depth/length\n",
        "label_map = {0: 'height', 1: 'width', 2: 'depth'}\n",
        "\n",
        "# Sample dimension keywords to classify\n",
        "dimension_keywords = ['height', 'h', 'width', 'w', 'length', 'l', 'depth', 'd', 'size']\n",
        "\n",
        "def classify_dimension(text):\n",
        "    # Tokenize input text and convert to tensor\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    \n",
        "    # Get model outputs (logits)\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(outputs.logits, dim=1)\n",
        "    \n",
        "    # Get the label with the highest probability\n",
        "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "    \n",
        "    # Convert label to corresponding dimension (height, width, depth)\n",
        "    return label_map[predicted_label]\n",
        "\n",
        "# Example text extracted from an image\n",
        "extracted_text = \"This product is 42cm in height, with a width of 50cm and depth of 30cm.\"\n",
        "\n",
        "# Split text into sentences for classification\n",
        "sentences = extracted_text.split(',')\n",
        "\n",
        "# Loop through sentences and classify each one\n",
        "for sentence in sentences:\n",
        "    dimension_class = classify_dimension(sentence)\n",
        "    print(f\"Sentence: '{sentence}' classified as: {dimension_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024/09/24 15:04:17] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/hammadkhan/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/hammadkhan/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/opt/anaconda3/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/hammadkhan/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
            "[2024/09/24 15:04:19] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.3735842704772949\n",
            "[2024/09/24 15:04:19] ppocr DEBUG: cls num  : 3, elapsed : 0.03380298614501953\n",
            "[2024/09/24 15:04:20] ppocr DEBUG: rec_res num  : 3, elapsed : 0.42775487899780273\n",
            "Processing image 1 of 20...\n",
            "[2024/09/24 15:04:21] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.05166983604431152\n",
            "[2024/09/24 15:04:21] ppocr DEBUG: cls num  : 6, elapsed : 0.04882621765136719\n",
            "[2024/09/24 15:04:22] ppocr DEBUG: rec_res num  : 6, elapsed : 1.0294270515441895\n",
            "Processing image 2 of 20...\n",
            "Processing image 3 of 20...\n",
            "Processing image 4 of 20...\n",
            "[2024/09/24 15:04:24] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.08800196647644043\n",
            "[2024/09/24 15:04:24] ppocr DEBUG: cls num  : 6, elapsed : 0.05078482627868652\n",
            "[2024/09/24 15:04:25] ppocr DEBUG: rec_res num  : 6, elapsed : 0.7270920276641846\n",
            "Processing image 5 of 20...\n",
            "Processing image 6 of 20...\n",
            "Processing image 7 of 20...\n",
            "[2024/09/24 15:04:26] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.3469529151916504\n",
            "[2024/09/24 15:04:26] ppocr DEBUG: cls num  : 3, elapsed : 0.025249242782592773\n",
            "[2024/09/24 15:04:27] ppocr DEBUG: rec_res num  : 3, elapsed : 0.36384010314941406\n",
            "Processing image 8 of 20...\n",
            "[2024/09/24 15:04:28] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.6778013706207275\n",
            "[2024/09/24 15:04:28] ppocr DEBUG: cls num  : 3, elapsed : 0.02631402015686035\n",
            "[2024/09/24 15:04:28] ppocr DEBUG: rec_res num  : 3, elapsed : 0.36324000358581543\n",
            "Processing image 9 of 20...\n",
            "[2024/09/24 15:04:29] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.7211987972259521\n",
            "[2024/09/24 15:04:29] ppocr DEBUG: cls num  : 3, elapsed : 0.07863283157348633\n",
            "[2024/09/24 15:04:30] ppocr DEBUG: rec_res num  : 3, elapsed : 0.3697688579559326\n",
            "Processing image 10 of 20...\n",
            "[2024/09/24 15:04:32] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.08622002601623535\n",
            "[2024/09/24 15:04:32] ppocr DEBUG: cls num  : 1, elapsed : 0.011001110076904297\n",
            "[2024/09/24 15:04:32] ppocr DEBUG: rec_res num  : 1, elapsed : 0.12681865692138672\n",
            "Processing image 11 of 20...\n",
            "[2024/09/24 15:04:34] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.13999199867248535\n",
            "[2024/09/24 15:04:34] ppocr DEBUG: cls num  : 3, elapsed : 0.026252031326293945\n",
            "[2024/09/24 15:04:34] ppocr DEBUG: rec_res num  : 3, elapsed : 0.374880313873291\n",
            "Processing image 12 of 20...\n",
            "[2024/09/24 15:04:35] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.35475683212280273\n",
            "[2024/09/24 15:04:35] ppocr DEBUG: cls num  : 4, elapsed : 0.036192893981933594\n",
            "[2024/09/24 15:04:36] ppocr DEBUG: rec_res num  : 4, elapsed : 0.5315370559692383\n",
            "Processing image 13 of 20...\n",
            "Processing image 14 of 20...\n",
            "Processing image 15 of 20...\n",
            "[2024/09/24 15:04:37] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.05949997901916504\n",
            "[2024/09/24 15:04:37] ppocr DEBUG: cls num  : 2, elapsed : 0.018312931060791016\n",
            "[2024/09/24 15:04:37] ppocr DEBUG: rec_res num  : 2, elapsed : 0.24533915519714355\n",
            "Processing image 16 of 20...\n",
            "[2024/09/24 15:04:37] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.07989978790283203\n",
            "[2024/09/24 15:04:37] ppocr DEBUG: cls num  : 9, elapsed : 0.07798480987548828\n",
            "[2024/09/24 15:04:38] ppocr DEBUG: rec_res num  : 9, elapsed : 1.0970706939697266\n",
            "Processing image 17 of 20...\n",
            "[2024/09/24 15:04:39] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.11287903785705566\n",
            "[2024/09/24 15:04:39] ppocr DEBUG: cls num  : 12, elapsed : 0.09890198707580566\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR\n",
        "import logging\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Configure logging for detailed debug information\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Unit normalization map to replace shorthand units with full forms\n",
        "unit_normalization_map = {\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'lbs': 'pound',\n",
        "    'lb': 'pound',\n",
        "    'ounces': 'ounce',\n",
        "    'oz': 'ounce',\n",
        "    'ml': 'milliliter',\n",
        "    'l': 'liter',\n",
        "    'cup': 'cup',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt',\n",
        "    'cm': 'centimeter',\n",
        "    'mm': 'millimeter',\n",
        "    'inches': 'inch',\n",
        "    'm': 'meter',\n",
        "    'km': 'kilometer',\n",
        "    'foot': 'foot',\n",
        "    'feet': 'foot'\n",
        "}\n",
        "\n",
        "# Pre-trained BERT model for dimension classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Labels: 0 = height, 1 = width, 2 = depth\n",
        "label_map = {0: 'height', 1: 'width', 2: 'depth'}\n",
        "\n",
        "# Reading CSV file\n",
        "def read_csv(file_path):\n",
        "    logging.info(f\"Reading CSV file: {file_path}\")\n",
        "    return pd.read_csv(file_path).head(20)\n",
        "\n",
        "# Normalising the units using the mapping provided above\n",
        "def normalize_unit(unit):\n",
        "    normalized = unit_normalization_map.get(unit.lower(), unit)\n",
        "    logging.debug(f\"Normalized unit '{unit}' to '{normalized}'\")\n",
        "    return normalized\n",
        "\n",
        "# Extracting numeric values without changing the format\n",
        "def format_value(value):\n",
        "    try:\n",
        "        return str(value)\n",
        "    except ValueError:\n",
        "        return value\n",
        "\n",
        "# Preprocessing the image to enhance OCR accuracy\n",
        "def preprocess_image(image):\n",
        "    image = resize_image(image)\n",
        "    image = enhance_contrast(image)\n",
        "    image = binarize_image(image)\n",
        "    image = denoise_image(image)\n",
        "    image = adaptive_threshold(image)\n",
        "    return image\n",
        "\n",
        "def resize_image(image, scale_factor=1.5):\n",
        "    width, height = image.size\n",
        "    resized_image = image.resize((int(width * scale_factor), int(height * scale_factor)), Image.Resampling.LANCZOS)\n",
        "    return resized_image\n",
        "\n",
        "def enhance_contrast(image):\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    enhanced_image = enhancer.enhance(2)\n",
        "    return enhanced_image\n",
        "\n",
        "def binarize_image(image):\n",
        "    gray = image.convert('L')\n",
        "    binary = gray.point(lambda x: 0 if x < 128 else 255, '1')\n",
        "    return binary\n",
        "\n",
        "def denoise_image(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    denoised_image = cv2.GaussianBlur(image_np, (5, 5), 0)\n",
        "    return Image.fromarray(denoised_image)\n",
        "\n",
        "def adaptive_threshold(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    thresh_image = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    return Image.fromarray(thresh_image)\n",
        "\n",
        "# Perform OCR with PaddleOCR\n",
        "def ocr_paddleocr(image, ocr_instance):\n",
        "    try:\n",
        "        result = ocr_instance.ocr(np.array(image), cls=True)\n",
        "        logging.debug(f\"OCR result: {result}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"OCR processing failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    if not result or not isinstance(result, list):\n",
        "        return \"\"\n",
        "\n",
        "    extracted_text = []\n",
        "    for block in result:\n",
        "        if isinstance(block, list) and len(block) > 0:\n",
        "            for line in block:\n",
        "                if isinstance(line, list) and len(line) > 1:\n",
        "                    text = line[1][0]\n",
        "                    extracted_text.append(text)\n",
        "\n",
        "    extracted_text_str = \"\\n\".join(extracted_text)\n",
        "    logging.debug(f\"Extracted text: {extracted_text_str}\")\n",
        "    return extracted_text_str if extracted_text else \"\"\n",
        "\n",
        "# Running OCR on the image\n",
        "def run_ocr(image, ocr_instance):\n",
        "    paddleocr_text = ocr_paddleocr(image, ocr_instance)\n",
        "    return {'text': paddleocr_text}\n",
        "\n",
        "# Classify text to determine dimension type\n",
        "def classify_dimension(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probabilities = F.softmax(outputs.logits, dim=1)\n",
        "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "    return label_map[predicted_label]\n",
        "\n",
        "# Extracting information using OCR and NLP-based dimension classification\n",
        "def extract_info(image_url, entity_name, ocr_instance, cache):\n",
        "    if image_url in cache:\n",
        "        cleaned_text = cache[image_url]\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            logging.info(f\"Loaded image from URL: {image_url}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load image from {image_url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "        img = preprocess_image(img)\n",
        "        ocr_results = run_ocr(img, ocr_instance)\n",
        "        cleaned_text = ocr_results['text']\n",
        "        cache[image_url] = cleaned_text\n",
        "\n",
        "    sentences = cleaned_text.split('\\n')\n",
        "    dimension_map = {'height': None, 'width': None, 'depth': None}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        dimension_class = classify_dimension(sentence)\n",
        "\n",
        "        match = re.search(r'(\\d+\\.?\\d*)\\s*(cm|mm|inch|feet|m)', sentence, re.IGNORECASE)\n",
        "        if match:\n",
        "            value = match.group(1)\n",
        "            unit = normalize_unit(match.group(2))\n",
        "            dimension_map[dimension_class] = f\"{value} {unit}\"\n",
        "\n",
        "    if entity_name in dimension_map and dimension_map[entity_name]:\n",
        "        return dimension_map[entity_name]\n",
        "    \n",
        "    return \"\"\n",
        "\n",
        "# Predicting entity values and save results to a CSV file\n",
        "def predict_and_save(csv_data, output_file_path, ocr_instance):\n",
        "    predictions = []\n",
        "    total_images = len(csv_data)\n",
        "    cache = {}\n",
        "\n",
        "    for index, row in csv_data.iterrows():\n",
        "        image_url = row['image_link']\n",
        "        entity_name = row['entity_name']\n",
        "\n",
        "        predicted_value = extract_info(image_url, entity_name, ocr_instance, cache)\n",
        "\n",
        "        if pd.isna(predicted_value) or predicted_value == \"\":\n",
        "            predicted_value = \"\"\n",
        "\n",
        "        print(f\"Processing image {index + 1} of {total_images}...\")\n",
        "\n",
        "        predictions.append({'index': index, 'prediction': predicted_value})\n",
        "\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Main function to load data and predict entity values\n",
        "def main():\n",
        "    csv_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test.csv'\n",
        "    output_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test_out_sample.csv'\n",
        "\n",
        "    csv_data = read_csv(csv_file_path)\n",
        "    ocr_instance = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n",
        "    predict_and_save(csv_data, output_file_path, ocr_instance)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR\n",
        "import logging\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Configure logging for detailed debug information\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Entity-unit mapping for validation\n",
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'depth': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'height': {'centimetre', 'cm', 'foot', 'ft', 'inch', 'in', 'metre', 'm', 'millimetre', 'mm', 'yard', 'yd'},\n",
        "    'item_weight': {'gram', 'g', 'kilogram', 'kg', 'microgram', 'μg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton'},\n",
        "    'maximum_weight_recommendation': {'gram', 'g', 'kilogram', 'kg', 'microgram', 'μg', 'milligram', 'mg', 'ounce', 'oz', 'pound', 'lb', 'ton'},\n",
        "    'voltage': {'kilovolt', 'kV', 'millivolt', 'mV', 'volt', 'V'},\n",
        "    'wattage': {'kilowatt', 'kW', 'watt', 'W'},\n",
        "    'item_volume': {'centilitre', 'cl', 'cubic foot', 'cu ft', 'cubic inch', 'cu in', 'cup', 'decilitre', 'dl', 'fluid ounce', 'fl oz', 'gallon', 'gal', 'imperial gallon', 'imp gal', 'litre', 'l', 'microlitre', 'μl', 'millilitre', 'ml', 'pint', 'pt', 'quart', 'qt'},\n",
        "    'frequency': {'hertz', 'Hz'},  # Including 'Hz' for frequency\n",
        "    'current': {'ampere', 'A'},  # Including 'A' for current\n",
        "}\n",
        "\n",
        "\n",
        "# Set of all allowed units\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n",
        "\n",
        "# Pre-trained BERT model for dimension classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6,)\n",
        "\n",
        "# Labels: 0 = height, 1 = width, 2 = depth\n",
        "label_map = {0: 'height', 1: 'width', 2: 'depth', 3: 'weight',\n",
        "    4: 'voltage',\n",
        "    5: 'current',}\n",
        "\n",
        "# Reading CSV file\n",
        "def read_csv(file_path):\n",
        "    logging.info(f\"Reading CSV file: {file_path}\")\n",
        "    return pd.read_csv(file_path).head(20)\n",
        "\n",
        "# Normalizing units using the mapping and allowed units\n",
        "def normalize_unit(unit):\n",
        "    normalized = unit_normalization_map.get(unit.lower(), unit)\n",
        "    if normalized in allowed_units:\n",
        "        logging.debug(f\"Normalized unit '{unit}' to '{normalized}'\")\n",
        "        return normalized\n",
        "    else:\n",
        "        logging.warning(f\"Invalid unit '{unit}' found.\")\n",
        "        return None\n",
        "\n",
        "# Extracting numeric values without changing the format\n",
        "def format_value(value):\n",
        "    try:\n",
        "        return str(value)\n",
        "    except ValueError:\n",
        "        return value\n",
        "\n",
        "# Preprocessing the image to enhance OCR accuracy\n",
        "def preprocess_image(image):\n",
        "    image = resize_image(image)\n",
        "    image = enhance_contrast(image)\n",
        "    image = binarize_image(image)\n",
        "    image = denoise_image(image)\n",
        "    image = adaptive_threshold(image)\n",
        "    return image\n",
        "\n",
        "def resize_image(image, scale_factor=1.5):\n",
        "    width, height = image.size\n",
        "    resized_image = image.resize((int(width * scale_factor), int(height * scale_factor)), Image.Resampling.LANCZOS)\n",
        "    return resized_image\n",
        "\n",
        "def enhance_contrast(image):\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    enhanced_image = enhancer.enhance(2)\n",
        "    return enhanced_image\n",
        "\n",
        "def binarize_image(image):\n",
        "    gray = image.convert('L')\n",
        "    binary = gray.point(lambda x: 0 if x < 128 else 255, '1')\n",
        "    return binary\n",
        "\n",
        "def denoise_image(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    denoised_image = cv2.GaussianBlur(image_np, (5, 5), 0)\n",
        "    return Image.fromarray(denoised_image)\n",
        "\n",
        "def adaptive_threshold(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    thresh_image = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    return Image.fromarray(thresh_image)\n",
        "\n",
        "# Perform OCR with PaddleOCR\n",
        "def ocr_paddleocr(image, ocr_instance):\n",
        "    try:\n",
        "        result = ocr_instance.ocr(np.array(image), cls=True)\n",
        "        logging.debug(f\"OCR result: {result}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"OCR processing failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    if not result or not isinstance(result, list):\n",
        "        return \"\"\n",
        "\n",
        "    extracted_text = []\n",
        "    for block in result:\n",
        "        if isinstance(block, list) and len(block) > 0:\n",
        "            for line in block:\n",
        "                if isinstance(line, list) and len(line) > 1:\n",
        "                    text = line[1][0]\n",
        "                    extracted_text.append(text)\n",
        "\n",
        "    extracted_text_str = \"\\n\".join(extracted_text)\n",
        "    logging.debug(f\"Extracted text: {extracted_text_str}\")\n",
        "    return extracted_text_str if extracted_text else \"\"\n",
        "\n",
        "# Running OCR on the image\n",
        "def run_ocr(image, ocr_instance):\n",
        "    paddleocr_text = ocr_paddleocr(image, ocr_instance)\n",
        "    return {'text': paddleocr_text}\n",
        "\n",
        "# Classify text to determine dimension type\n",
        "def classify_dimension(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probabilities = F.softmax(outputs.logits, dim=1)\n",
        "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "    return label_map[predicted_label]\n",
        "\n",
        "# Extracting information using OCR, entity-unit mapping, and NLP-based dimension classification\n",
        "# Extracting information using OCR, entity-unit mapping, and NLP-based dimension classification\n",
        "def extract_info(image_url, entity_name, ocr_instance, cache):\n",
        "    if image_url in cache:\n",
        "        cleaned_text = cache[image_url]\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            logging.info(f\"Loaded image from URL: {image_url}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load image from {image_url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "        img = preprocess_image(img)\n",
        "        ocr_results = run_ocr(img, ocr_instance)\n",
        "        cleaned_text = ocr_results['text']\n",
        "        cache[image_url] = cleaned_text\n",
        "\n",
        "    sentences = cleaned_text.split('\\n')\n",
        "    dimension_map = {label: None for label in label_map.values()}  # Dynamically handles all dimensions\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Classify the dimension for each sentence\n",
        "        dimension_class = classify_dimension(sentence)\n",
        "\n",
        "        # Use regex to find value and unit\n",
        "        match = re.search(r'(\\d+\\.?\\d*)\\s*(\\w+)', sentence, re.IGNORECASE)\n",
        "        if match:\n",
        "            value = match.group(1)\n",
        "            unit = normalize_unit(match.group(2))\n",
        "\n",
        "            # Check if the detected unit is valid for the classified dimension\n",
        "            if unit and unit in entity_unit_map.get(dimension_class, {}):\n",
        "                dimension_map[dimension_class] = f\"{value} {unit}\"\n",
        "\n",
        "    if entity_name in dimension_map and dimension_map[entity_name]:\n",
        "        return dimension_map[entity_name]\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Predicting entity values and save results to a CSV file\n",
        "def predict_and_save(csv_data, output_file_path, ocr_instance):\n",
        "    predictions = []\n",
        "    total_images = len(csv_data)\n",
        "    cache = {}\n",
        "\n",
        "    for index, row in csv_data.iterrows():\n",
        "        image_url = row['image_link']\n",
        "        entity_name = row['entity_name']\n",
        "\n",
        "        predicted_value = extract_info(image_url, entity_name, ocr_instance, cache)\n",
        "\n",
        "        if pd.isna(predicted_value) or predicted_value == \"\":\n",
        "            predicted_value = \"\"\n",
        "\n",
        "        print(f\"Processing image {index + 1} of {total_images}...\")\n",
        "\n",
        "        predictions.append({'index': index, 'prediction': predicted_value})\n",
        "\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Main function to load data and predict entity values\n",
        "def main():\n",
        "    csv_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test.csv'\n",
        "    output_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test_out_sample.csv'\n",
        "\n",
        "    csv_data = read_csv(csv_file_path)\n",
        "    ocr_instance = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n",
        "    predict_and_save(csv_data, output_file_path, ocr_instance)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the necessary dependencies and required libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR\n",
        "import logging\n",
        "\n",
        "# Configure logging for detailed debug information\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Unit normalization map to replace shorthand units with full forms\n",
        "unit_normalization_map = {\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'lbs': 'pound',\n",
        "    'lb': 'pound',\n",
        "    'ounces': 'ounce',\n",
        "    'oz': 'ounce',\n",
        "    'ml': 'milliliter',\n",
        "    'l': 'liter',\n",
        "    'cup': 'cup',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt',\n",
        "    'cm': 'centimeter',\n",
        "    'mm': 'millimeter',\n",
        "    'inches': 'inch',\n",
        "    'm': 'meter',\n",
        "    'km': 'kilometer',\n",
        "    'foot': 'foot',\n",
        "    'feet': 'foot'\n",
        "}\n",
        "\n",
        "# Reading CSV file\n",
        "def read_csv(file_path):\n",
        "    logging.info(f\"Reading CSV file: {file_path}\")\n",
        "    return pd.read_csv(file_path).tail(100)\n",
        "\n",
        "# Normalising the units using the mapping provided above\n",
        "def normalize_unit(unit):\n",
        "    normalized = unit_normalization_map.get(unit.lower(), unit)\n",
        "    logging.debug(f\"Normalized unit '{unit}' to '{normalized}'\")\n",
        "    return normalized\n",
        "\n",
        "# Extracting numeric values without changing the format\n",
        "def format_value(value):\n",
        "    try:\n",
        "        return str(value)\n",
        "    except ValueError:\n",
        "        return value\n",
        "\n",
        "# Preprocessing the image to enhance OCR accuracy\n",
        "def preprocess_image(image):\n",
        "    image = resize_image(image)\n",
        "    image = enhance_contrast(image)\n",
        "    image = binarize_image(image)\n",
        "    image = denoise_image(image)\n",
        "    image = adaptive_threshold(image)\n",
        "    return image\n",
        "\n",
        "# Resizing the image for better OCR accuracy\n",
        "def resize_image(image, scale_factor=1.5):\n",
        "    width, height = image.size\n",
        "    resized_image = image.resize((int(width * scale_factor), int(height * scale_factor)), Image.Resampling.LANCZOS)\n",
        "    return resized_image\n",
        "\n",
        "# Enhancing the contrast to improve text visibility\n",
        "def enhance_contrast(image):\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    enhanced_image = enhancer.enhance(2)\n",
        "    return enhanced_image\n",
        "\n",
        "# Binarizing the image to simplify OCR (black-and-white)\n",
        "def binarize_image(image):\n",
        "    gray = image.convert('L')\n",
        "    binary = gray.point(lambda x: 0 if x < 128 else 255, '1')\n",
        "    return binary\n",
        "\n",
        "# Denoising the image using Gaussian blur\n",
        "def denoise_image(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    denoised_image = cv2.GaussianBlur(image_np, (5, 5), 0)\n",
        "    return Image.fromarray(denoised_image)\n",
        "\n",
        "# Apply adaptive thresholding to the image\n",
        "def adaptive_threshold(image):\n",
        "    image_np = np.array(image.convert('L'))\n",
        "    thresh_image = cv2.adaptiveThreshold(image_np, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    return Image.fromarray(thresh_image)\n",
        "\n",
        "# Perform OCR with PaddleOCR\n",
        "def ocr_paddleocr(image, ocr_instance):\n",
        "    try:\n",
        "        result = ocr_instance.ocr(np.array(image), cls=True)\n",
        "        logging.debug(f\"OCR result: {result}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"OCR processing failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    if not result or not isinstance(result, list):\n",
        "        return \"\"\n",
        "\n",
        "    extracted_text = []\n",
        "    for block in result:\n",
        "        if isinstance(block, list) and len(block) > 0:\n",
        "            for line in block:\n",
        "                if isinstance(line, list) and len(line) > 1:\n",
        "                    text = line[1][0]\n",
        "                    extracted_text.append(text)\n",
        "\n",
        "    extracted_text_str = \" \".join(extracted_text)  # Joining extracted text into a single line\n",
        "    logging.debug(f\"Extracted text: {extracted_text_str}\")\n",
        "    return extracted_text_str if extracted_text else \"\"\n",
        "\n",
        "# Running OCR on the image\n",
        "def run_ocr(image, ocr_instance):\n",
        "    paddleocr_text = ocr_paddleocr(image, ocr_instance)\n",
        "    return {'text': paddleocr_text}\n",
        "\n",
        "# Extracting information using OCR and regex\n",
        "def extract_info(image_url, entity_name, ocr_instance, cache):\n",
        "    if image_url in cache:\n",
        "        cleaned_text = cache[image_url]\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            logging.info(f\"Loaded image from URL: {image_url}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load image from {image_url}: {e}\")\n",
        "            return \"\", \"\"\n",
        "\n",
        "        # Preprocessing the image\n",
        "        img = preprocess_image(img)\n",
        "\n",
        "        # Performing OCR\n",
        "        ocr_results = run_ocr(img, ocr_instance)\n",
        "        cleaned_text = ocr_results['text']\n",
        "        cache[image_url] = cleaned_text\n",
        "\n",
        "    # Defining regex patterns\n",
        "    regex_patterns = {\n",
        "        'item_weight': r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)',\n",
        "        'item_volume': r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|fl\\.?\\s*oz|cup|liter|millilitre|gallon?)',\n",
        "        'voltage': r'(\\d+(?:\\.\\d+)?)\\s*(v|volt|mv|millivolt)',\n",
        "        'wattage': r'(\\d+(?:\\.\\d+)?)\\s*(w|watt|mw|milliwatt)',\n",
        "        'maximum_weight_recommendation': r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)',\n",
        "        'height': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "        'depth': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "        'width': r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)',\n",
        "    }\n",
        "\n",
        "    pattern = regex_patterns.get(entity_name)\n",
        "    if pattern:\n",
        "        match = re.search(pattern, cleaned_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            value = format_value(match.group(1))\n",
        "            unit = normalize_unit(match.group(2))\n",
        "            logging.info(f\"Extracted entity {entity_name}: {value} {unit}\")\n",
        "            return cleaned_text, f\"{value} {unit}\"\n",
        "\n",
        "    return cleaned_text, \"\"\n",
        "\n",
        "# Predicting entity values and save results to a CSV file\n",
        "def predict_and_save(csv_data, output_file_path, ocr_instance):\n",
        "    predictions = []\n",
        "    cache = {}\n",
        "\n",
        "    for index, row in csv_data.iterrows():\n",
        "        image_url = row['image_link']\n",
        "        entity_name = row['entity_name']\n",
        "        entity_value = row['entity_value']  # Keep the original entity_value\n",
        "\n",
        "        # Extract OCR text and predicted value\n",
        "        extracted_text, predicted_value = extract_info(image_url, entity_name, ocr_instance, cache)\n",
        "\n",
        "        if pd.isna(predicted_value) or predicted_value == \"\":\n",
        "            predicted_value = entity_value  # Use original entity_value if prediction is unavailable\n",
        "\n",
        "        # Append the results\n",
        "        predictions.append({\n",
        "            'extracted_text': extracted_text,  # First column with single-line extracted text\n",
        "            'entity_name': entity_name,  # Keep entity_name as it is\n",
        "            'entity_value': entity_value  # Keep entity_value as it is\n",
        "        })\n",
        "\n",
        "        logging.info(f\"Processed {index + 1} / {len(csv_data)}\")\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.to_csv(output_file_path, index=False)\n",
        "    logging.info(f\"Predicted values and extracted texts saved to {output_file_path}\")\n",
        "\n",
        "# Main function to load data and predict entity values\n",
        "def main():\n",
        "    csv_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/train.csv'\n",
        "    output_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts.csv'\n",
        "\n",
        "    csv_data = read_csv(csv_file_path)\n",
        "\n",
        "    # Initialize PaddleOCR once\n",
        "    ocr_instance = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n",
        "\n",
        "    # Predicting entity values and save them to the output CSV\n",
        "    predict_and_save(csv_data, output_file_path, ocr_instance)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OPTIMIZED APPROACH TO EXTRACT TEXTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR\n",
        "import logging\n",
        "\n",
        "# Configure logging for detailed debug information\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Unit normalization map to replace shorthand units with full forms\n",
        "unit_normalization_map = {\n",
        "    'g': 'gram',\n",
        "    'kg': 'kilogram',\n",
        "    'lbs': 'pound',\n",
        "    'lb': 'pound',\n",
        "    'ounces': 'ounce',\n",
        "    'oz': 'ounce',\n",
        "    'ml': 'milliliter',\n",
        "    'l': 'liter',\n",
        "    'v': 'volt',\n",
        "    'w': 'watt',\n",
        "    'cm': 'centimeter',\n",
        "    'mm': 'millimeter',\n",
        "    'inches': 'inch',\n",
        "    'm': 'meter',\n",
        "    'km': 'kilometer',\n",
        "    'foot': 'foot',\n",
        "    'feet': 'foot'\n",
        "}\n",
        "\n",
        "# Pre-compiled regex patterns to optimize extraction\n",
        "regex_patterns = {\n",
        "    'item_weight': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)', re.IGNORECASE),\n",
        "    'item_volume': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|fl\\.?\\s*oz|cup|liter|millilitre|gallon?)', re.IGNORECASE),\n",
        "    'voltage': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(v|volt|mv|millivolt)', re.IGNORECASE),\n",
        "    'wattage': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(w|watt|mw|milliwatt)', re.IGNORECASE),\n",
        "    'maximum_weight_recommendation': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(g|kg|lb?|ounce|gram|pound?)', re.IGNORECASE),\n",
        "    'height': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)', re.IGNORECASE),\n",
        "    'depth': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)', re.IGNORECASE),\n",
        "    'width': re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(cm|mm|inch|metre|m|foot|feet)', re.IGNORECASE),\n",
        "}\n",
        "\n",
        "# Simplified function for reading the CSV\n",
        "def read_csv(file_path):\n",
        "    logging.info(f\"Reading CSV file: {file_path}\")\n",
        "    return pd.read_csv(file_path).tail(100)\n",
        "\n",
        "# Normalizing the units using the mapping provided above\n",
        "def normalize_unit(unit):\n",
        "    return unit_normalization_map.get(unit.lower(), unit)\n",
        "\n",
        "# Simplified image preprocessing for speed\n",
        "def preprocess_image(image):\n",
        "    # Resize and enhance contrast only if image size exceeds threshold\n",
        "    width, height = image.size\n",
        "    if width > 1000 or height > 1000:\n",
        "        image = image.resize((int(width * 0.5), int(height * 0.5)), Image.Resampling.LANCZOS)\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    return enhancer.enhance(1.5)\n",
        "\n",
        "# Perform OCR with PaddleOCR\n",
        "def ocr_paddleocr(image, ocr_instance):\n",
        "    try:\n",
        "        result = ocr_instance.ocr(np.array(image), cls=True)\n",
        "        extracted_text = \" \".join([line[1][0] for block in result for line in block if line[1]])\n",
        "        logging.debug(f\"OCR result: {extracted_text}\")\n",
        "        return extracted_text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"OCR processing failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Simplified OCR extraction logic with caching\n",
        "def extract_info(image_url, entity_name, ocr_instance, cache):\n",
        "    if image_url in cache:\n",
        "        return cache[image_url], cache[image_url]\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load image from {image_url}: {e}\")\n",
        "        return \"\", \"\"\n",
        "\n",
        "    img = preprocess_image(img)\n",
        "    ocr_text = ocr_paddleocr(img, ocr_instance)\n",
        "    cache[image_url] = ocr_text\n",
        "\n",
        "    pattern = regex_patterns.get(entity_name)\n",
        "    if pattern:\n",
        "        match = pattern.search(ocr_text)\n",
        "        if match:\n",
        "            value, unit = match.group(1), normalize_unit(match.group(2))\n",
        "            logging.info(f\"Extracted entity {entity_name}: {value} {unit}\")\n",
        "            return ocr_text, f\"{value} {unit}\"\n",
        "    \n",
        "    return ocr_text, \"\"\n",
        "\n",
        "# Predict and save results in batch\n",
        "def predict_and_save(csv_data, output_file_path, ocr_instance):\n",
        "    predictions = []\n",
        "    cache = {}\n",
        "\n",
        "    for _, row in csv_data.iterrows():\n",
        "        image_url = row['image_link']\n",
        "        entity_name = row['entity_name']\n",
        "        entity_value = row['entity_value']\n",
        "\n",
        "        ocr_text, predicted_value = extract_info(image_url, entity_name, ocr_instance, cache)\n",
        "        if not predicted_value:\n",
        "            predicted_value = entity_value\n",
        "\n",
        "        predictions.append({\n",
        "            'extracted_text': ocr_text,\n",
        "            'entity_name': entity_name,\n",
        "            'entity_value': entity_value\n",
        "        })\n",
        "\n",
        "    pd.DataFrame(predictions).to_csv(output_file_path, index=False)\n",
        "    logging.info(f\"Predicted values and extracted texts saved to {output_file_path}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    csv_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/train.csv'\n",
        "    output_file_path = '/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv'\n",
        "\n",
        "    csv_data = read_csv(csv_file_path)\n",
        "    ocr_instance = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False)  # Use GPU only if necessary\n",
        "    predict_and_save(csv_data, output_file_path, ocr_instance)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch pandas scikit-learn datasets tf-keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load the CSV file (columns: 'extracted_text', 'entity_name', 'entity_value')\n",
        "df = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "\n",
        "# Convert entity_name to numeric labels (for classification)\n",
        "df['entity_name_label'] = df['entity_name'].factorize()[0]\n",
        "\n",
        "# Ensure all text data is string and handle NaN values\n",
        "df['extracted_text'] = df['extracted_text'].fillna(\"\").astype(str)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['extracted_text'].tolist(),\n",
        "    df['entity_name_label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data for both training and validation sets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert labels to torch tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# Create a custom PyTorch dataset\n",
        "class ExtractedTextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Create PyTorch datasets for training and validation\n",
        "train_dataset = ExtractedTextDataset(train_encodings, train_labels)\n",
        "val_dataset = ExtractedTextDataset(val_encodings, val_labels)\n",
        "\n",
        "# Now you can proceed to load these datasets into a DataLoader and train your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_encodings' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m item\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create PyTorch datasets\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ExtractedTextDataset(train_encodings, train_labels)\n\u001b[1;32m     18\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ExtractedTextDataset(val_encodings, val_labels)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_encodings' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ExtractedTextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = ExtractedTextDataset(train_encodings, train_labels)\n",
        "val_dataset = ExtractedTextDataset(val_encodings, val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[54], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply tokenization to the 'text' column\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_function)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Convert to a list of dictionaries for training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label} \n\u001b[1;32m     20\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m item, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
            "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "Cell \u001b[0;32mIn[54], line 13\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(text):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3117\u001b[0m     )\n\u001b[1;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3123\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset using pandas\n",
        "df = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Adjust num_labels based on your dataset\n",
        "\n",
        "# Tokenize the dataset row by row\n",
        "def preprocess_function(text):\n",
        "    return tokenizer(text, padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# Apply tokenization to the 'text' column\n",
        "df['tokenized_text'] = df['extracted_text'].apply(preprocess_function)\n",
        "\n",
        "# Convert to a list of dictionaries for training\n",
        "train_data = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask'], 'labels': label} \n",
        "              for item, label in zip(df['tokenized_text'], df['labels'])]\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(train_data)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/Users/hammadkhan/Downloads/student_resource 3/data',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=5,\n",
        "    per_device_eval_batch_size=5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Split the tokenized dataset into train and test sets\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m val_encodings \u001b[38;5;241m=\u001b[39m tokenizer(val_texts, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Convert to Hugging Face Dataset format\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label} \n\u001b[1;32m     37\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m item, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_encodings, train_labels)]\n\u001b[1;32m     39\u001b[0m val_data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label} \n\u001b[1;32m     40\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m item, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(val_encodings, val_labels)]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create datasets from dicts\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "\n",
        "# Convert entity_name to numeric labels (for classification)\n",
        "df['entity_name_label'] = df['entity_name'].factorize()[0]\n",
        "\n",
        "# Ensure all text data is string and handle NaN values\n",
        "df['extracted_text'] = df['extracted_text'].fillna(\"\").astype(str)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['extracted_text'].tolist(),\n",
        "    df['entity_name_label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset row by row\n",
        "def preprocess_function(text):\n",
        "    return tokenizer(text, padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# Tokenize the text data for both training and validation sets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask'], 'labels': label} \n",
        "              for item, label in zip(train_encodings, train_labels)]\n",
        "\n",
        "val_data = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask'], 'labels': label} \n",
        "              for item, label in zip(val_encodings, val_labels)]\n",
        "\n",
        "# Create datasets from dicts\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n",
        "\n",
        "# Initialize the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['entity_name_label'].unique()))  # Adjust num_labels\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/Users/hammadkhan/Downloads/student_resource 3/data',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/Users/hammadkhan/Downloads/student_resource 3/logs',  # Where logs are stored\n",
        "    logging_steps=10,\n",
        "    save_steps=10\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19b92f9380264aaebc57e4519caf933c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "\n",
        "# Convert entity_name to numeric labels (for classification)\n",
        "df['entity_name_label'] = df['entity_name'].factorize()[0]\n",
        "\n",
        "# Ensure all text data is string and handle NaN values\n",
        "df['extracted_text'] = df['extracted_text'].fillna(\"\").astype(str)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['extracted_text'].tolist(),\n",
        "    df['entity_name_label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data for both training and validation sets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "# Add labels to the tokenized encodings\n",
        "train_encodings['labels'] = torch.tensor(train_labels)\n",
        "val_encodings['labels'] = torch.tensor(val_labels)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({key: val.tolist() for key, val in train_encodings.items()})\n",
        "val_dataset = Dataset.from_dict({key: val.tolist() for key, val in val_encodings.items()})\n",
        "\n",
        "# Initialize the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['entity_name_label'].unique()))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "  \n",
        "    output_dir='/Users/hammadkhan/Downloads/student_resource 3/data',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=2,  # Reduce to save memory\n",
        "    per_device_eval_batch_size=2,   # Reduce to save memory\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/Users/hammadkhan/Downloads/student_resource 3',\n",
        "    logging_steps=10,\n",
        "    save_steps=10,\n",
        "    gradient_accumulation_steps=4,  # Accumulates gradients for larger effective batch size\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ],
      "source": [
        "df['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/nj/923tjm6x6xg896dhx3mt1jpr0000gn/T/ipykernel_82516/2904798698.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pip show transformers torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extracted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'type'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(token_labels)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences, labels\n\u001b[0;32m---> 32\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m preprocess_data(data)\n",
            "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m entity_value \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_value\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize the extracted text\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m extracted_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Assign labels based on entity_name and entity_value\u001b[39;00m\n\u001b[1;32m     20\u001b[0m token_labels \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "data['entity_value']=data['entity_value'].astype(str)\n",
        "# Preprocess the data for training\n",
        "def preprocess_data(data):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        extracted_text = row['extracted_text']\n",
        "        entity_name = row['entity_name']\n",
        "        entity_value = row['entity_value']\n",
        "\n",
        "        # Tokenize the extracted text\n",
        "        tokens = extracted_text.split()\n",
        "\n",
        "        # Assign labels based on entity_name and entity_value\n",
        "        token_labels = []\n",
        "        for token in tokens:\n",
        "            if entity_name in token:\n",
        "                token_labels.append('B-DIMENSION')  # Begin of dimension\n",
        "            else:\n",
        "                token_labels.append('O')  # Outside of dimension\n",
        "        \n",
        "        sentences.append(tokens)\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "sentences, labels = preprocess_data(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(token_labels)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences, labels\n\u001b[0;32m---> 32\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m preprocess_data(data)\n",
            "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m entity_value \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_value\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize the extracted text\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m extracted_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Assign labels based on entity_name and entity_value\u001b[39;00m\n\u001b[1;32m     20\u001b[0m token_labels \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/Users/hammadkhan/Downloads/student_resource 3/dataset/test_extracted_texts_opt.csv')\n",
        "\n",
        "# Preprocess the data for training\n",
        "def preprocess_data(data):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        extracted_text = row['extracted_text']\n",
        "        entity_name = row['entity_name']\n",
        "        entity_value = row['entity_value']\n",
        "\n",
        "        # Tokenize the extracted text\n",
        "        tokens = extracted_text.split()\n",
        "\n",
        "        # Assign labels based on entity_name and entity_value\n",
        "        token_labels = []\n",
        "        for token in tokens:\n",
        "            if entity_name in token:\n",
        "                token_labels.append('B-DIMENSION')  # Begin of dimension\n",
        "            else:\n",
        "                token_labels.append('O')  # Outside of dimension\n",
        "        \n",
        "        sentences.append(tokens)\n",
        "        labels.append(token_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "sentences, labels = preprocess_data(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)  # O and B-DIMENSION\n",
        "\n",
        "# Tokenize the sentences\n",
        "train_encodings = tokenizer(sentences, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "\n",
        "# Convert labels into token-level labels\n",
        "def align_labels_with_tokens(labels, encodings):\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(labels):\n",
        "        word_ids = encodings.word_ids(batch_index=i)\n",
        "        aligned_labels.append([label[word_id] if word_id is not None else -100 for word_id in word_ids])\n",
        "    return aligned_labels\n",
        "\n",
        "train_labels = align_labels_with_tokens(labels, train_encodings)\n",
        "\n",
        "# Convert data to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels\n",
        "})\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_dimension(extracted_text):\n",
        "    inputs = tokenizer(extracted_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model(**inputs).logits\n",
        "    predictions = outputs.argmax(dim=-1)\n",
        "    predicted_labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
        "    \n",
        "    # Map tokens back to the predicted labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    return list(zip(tokens, predicted_labels))\n",
        "\n",
        "# Example usage\n",
        "extracted_text = \"66-89cm 12cm 3cm 2m 3.6cm 16.5cm\"\n",
        "predicted = predict_dimension(extracted_text)\n",
        "print(predicted)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
